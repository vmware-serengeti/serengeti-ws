# serengeti bootup configurations, updated by firstboot script
serengeti.uuid = xxx-uuid
serengeti.initialize.uuid = true

# root vm folder for all clusters will be SERENGETI-CLUSTER-${serengeti.uuid}
serengeti.root_folder_prefix = SERENGETI-vApp

# Turn on intensive checks in debug mode (including AuAssert checks)
# Note: the debug code should not have side-effect on the outside code,
# i.e. turning off debug should not leads to changes of code logic
serengeti.debug = true

# the max number of event processors to handle vCenter events
serengeti.event_processor.poolsize = 8

# the number of concurrent clone a template should support
serengeti.singlevm.concurrency = 1

# also means max number of nodes a cluster can have
serengeti.scheduler.poolsize = 1024

# version field
serengeti.version = 2.2.0

# DAL transaction random rollback, i.e. deadlock simulation
# only valid when serengeti.debug = true
dal.stressTxnRollback = true

# the template vm name, only used when deploy_as_vapp = false
template_vm_name = hadoop-template

# Is Serengeti deployed as an bundle vApp or two separeted VMs?
deploy_as_vapp = true

# initialize serengeti server with reousrces from the serengeti server VM?
# include the datastore, resource pool, network
init_resource = false

serengeti.distro_root = http://localhost/distros

# Turn on http proxy if the Serengeti Server needs a http proxy to connect to the Internet
# The wildcard doesn't work for 'serengeti.no_proxy'
#serengeti.http_proxy = http://proxy.domain.com:port
#serengeti.no_proxy = xyz.domain.com, 10.x.y.z, 192.168.x.y

# Hadoop Distro Vendors
# GPHD => GreenPlum HD, PHD => Pivotal HD, HDP => Hortonworks Data Platform, CDH => Cloudera Hadoop, MAPR => MapR
serengeti.distro_vendor = GENERIC, APACHE, BIGTOP, GPHD, PHD, HDP, CDH, MAPR

#Thrift server information
management.thrift.server = localhost
management.thrift.port = 9090
management.thrift.mock = false

# task configurations
task.enable_mq = true
task.threadpool.workers = 20
task.threadpool.queue_size = 50
task.rabbitmq.host = localhost
task.rabbitmq.port = 5672
task.rabbitmq.username =
task.rabbitmq.password =
task.rabbitmq.exchange = bddtask
task.rabbitmq.routekey_fmt = task.${task_id}
task.rabbitmq.recv_timeout_ms = 1000
task.rabbitmq.keepalive_time_ms = 10000

runtime.rabbitmq.exchange = bdd.runtime
runtime.rabbitmq.send.routekey = command


# storage size configuration (GB)
# master group represents the group contains hadoop_namenode or hadoop_jobtracker role or hbase_master role.
# worker group represents the group contains hadoop_datanode or hadoop_tasktracker role or hbase_regionserver role.
# client group represents the group contains hadoop_client or pig, hive role, or hbase_client role.
# zookeeper group represents the group contains zookeeper role.
# the last field represents the instance type, XL, L, M, and S.
storage.mastergroup.extralarge = 200
storage.mastergroup.large = 100
storage.mastergroup.medium = 50
storage.mastergroup.small = 25
storage.workergroup.extralarge = 400
storage.workergroup.large = 200
storage.workergroup.medium = 100
storage.workergroup.small = 50
storage.clientgroup.extralarge = 400
storage.clientgroup.large = 200
storage.clientgroup.medium = 100
storage.clientgroup.small = 50
storage.zookeepergroup.extralarge = 120
storage.zookeepergroup.large = 80
storage.zookeepergroup.medium = 40
storage.zookeepergroup.small = 20

elastic_runtime.automation.enable = false
