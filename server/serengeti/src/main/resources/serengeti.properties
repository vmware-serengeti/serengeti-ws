# serengeti bootup configurations, updated by firstboot script
serengeti.uuid = xxx-uuid
serengeti.initialize.uuid = true

# root vm folder for all clusters will be SERENGETI-CLUSTER-${serengeti.uuid}
serengeti.root_folder_prefix = SERENGETI-vApp

# Turn on intensive checks in debug mode (including AuAssert checks)
# Note: the debug code should not have side-effect on the outside code,
# i.e. turning off debug should not leads to changes of code logic
serengeti.debug = true

# the max number of event processors to handle vCenter events
serengeti.event_processor.poolsize = 8

# the number of concurrent clone a template should support
serengeti.singlevm.concurrency = 1

# also means max number of nodes a cluster can have
serengeti.scheduler.poolsize = 1024

# version field
serengeti.version = 2.1.1

# DAL transaction random rollback, i.e. deadlock simulation
# only valid when serengeti.debug = true
dal.stressTxnRollback = true

# the template vm name, only used when deploy_as_vapp = false
template_vm_name = node-template

# Is Serengeti deployed as an bundle vApp or two separeted VMs?
deploy_as_vapp = true

# initialize serengeti server with reousrces from the serengeti server VM?
# include the datastore, resource pool, network
init_resource = false

serengeti.distro_root = http://localhost/distros
serengeti.just_upgraded = false

# Turn on http proxy if the Serengeti Server needs a http proxy to connect to the Internet
# The wildcard doesn't work for 'serengeti.no_proxy'
#serengeti.http_proxy = http://proxy.domain.com:port
#serengeti.no_proxy = xyz.domain.com, 10.x.y.z, 192.168.x.y

# Hadoop Distro Vendors
# GENERIC => User Customized Distro Vendor, GPHD => GreenPlum HD, PHD => Pivotal HD,
# HDP => Hortonworks Data Platform, CDH => Cloudera Hadoop, MAPR => MapR, BIGTOP => Apache Bigtop
serengeti.distro_vendor = GENERIC, APACHE, BIGTOP, GPHD, PHD, HDP, CDH, MAPR, KUBERNETES, MESOS

#Thrift server information
management.thrift.server = localhost
management.thrift.port = 9090
management.thrift.mock = false

# task configurations
task.enable_mq = true
task.threadpool.workers = 20
task.threadpool.queue_size = 50
task.rabbitmq.host = localhost
task.rabbitmq.port = 5672
task.rabbitmq.username =
task.rabbitmq.password =
task.rabbitmq.exchange = bddtask
task.rabbitmq.routekey_fmt = task.${task_id}
task.rabbitmq.recv_timeout_ms = 1000
task.rabbitmq.keepalive_time_ms = 10000

runtime.rabbitmq.exchange = bdd.runtime
runtime.rabbitmq.send.routekey = command

# storage size configuration (GB)
# master group represents the group contains hadoop_namenode or hadoop_jobtracker role or hbase_master role.
# worker group represents the group contains hadoop_datanode or hadoop_tasktracker role or hbase_regionserver role.
# client group represents the group contains hadoop_client or pig, hive role, or hbase_client role.
# zookeeper group represents the group contains zookeeper role.
# the last field represents the instance type, XL, L, M, and S.
storage.mastergroup.extralarge = 200
storage.mastergroup.large = 100
storage.mastergroup.medium = 50
storage.mastergroup.small = 25
storage.workergroup.extralarge = 400
storage.workergroup.large = 200
storage.workergroup.medium = 100
storage.workergroup.small = 50
storage.clientgroup.extralarge = 400
storage.clientgroup.large = 200
storage.clientgroup.medium = 100
storage.clientgroup.small = 50
storage.zookeepergroup.extralarge = 120
storage.zookeepergroup.large = 80
storage.zookeepergroup.medium = 40
storage.zookeepergroup.small = 20

elastic_runtime.automation.enable = false

serengeti.clone.mode = linkedclone
serengeti.concurrent.job.enabled = true
serengeti.use.default.password = true

appmanager.types = ClouderaManager, Ambari
appmanager.factoryclass.ClouderaManager = com.vmware.bdd.plugin.clouderamgr.service.ClouderaManagerFactory
appmanager.factoryclass.Ambari = com.vmware.bdd.plugin.ambari.service.AmbariFactory
appmanager.service.connect.timeout.seconds = 30

ambari.hbase_depend_on_mapreduce = true

# milliseconds between each retry of ssh. This configuration together with the next one serengeti.ssh.max.retry.times
# is used to control how long we wait for sshd ready on the deployed vm
serengeti.ssh.sleep.time.before.retry = 3000

# 3000 * 600 = 1800,000 ms = 1800 s = 30 minutes, we set the default longest wait time to 30 minutes to handle most
# cases. If user is deploying a very large cluster, they may consider to increase serengeti.ssh.sleep.time.before.retry
# and serengeti.ssh.max.retry.times
serengeti.ssh.max.retry.times = 600

